{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zuoyu2524/Cycle-GAN/blob/main/LabSession6_RNN_L63_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QwnJfqggAJ6"
      },
      "source": [
        "# Lab session 6: Recurrent Neural Networks (RNN)\n",
        "\n",
        "Authors: [pierre.tandeo@imt-atlantique.fr](pierre.tandeo@imt-atlantique.fr), [lucas.drumetz@imt-atlantique.fr](lucas.drumetz@imt-atlantique.fr), [claire.scavinner-dorval@imt-atlantique.fr](claire.scavinner-dorval@imt-atlantique.fr), [sarah.reynaud@imt-atlantique.fr](sarah.reynaud@imt-atlantique.fr), [arthur.avenas@ifremer.fr](arthur.avenas@ifremer.fr)\n",
        "\n",
        "Year: 2022-2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASheZ9RgklPx"
      },
      "source": [
        "Student 1: ### TO DO ###\n",
        "\n",
        "Student 2: ### TO DO ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lDjOZUsgAJ-"
      },
      "source": [
        "In this practice, we will use a Recurrent Neural Networks (RNN) to fit multivariate time series and to make some predictions in the future. We will use the Lorenz-63 chaotic model, also known as the strange attractor (see https://en.wikipedia.org/wiki/Lorenz_system for more details).\n",
        "\n",
        "First, we will generate and visualize trajectories from the Lorenz-63 system. From these data, we will build training and test datasets. Then, we will adjust two models to fit the data: a classic linear autoregressive model and the specific LSTM (Long Short-Term Memory) architecture of RNN. Finally, we will compare these models in terms of prediction and simulation of new trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s51GKxngAKA"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "%pylab inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Parameters (figure size)\n",
        "pylab.rcParams['figure.figsize'] = (16, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eenxlTVH4EX7"
      },
      "source": [
        "**A few words about the use of GPUs**\n",
        "\n",
        "In order to accelerate the optimization of your LSTM, we suggest to use GPUs. In Google Colab, you will need to enable GPUs for the notebook:\n",
        "* navigate to Edit → Notebook Settings\n",
        "* select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Then, the use of GPU in PyTorch is based on four steps:\n",
        "* declare the GPU device (cell below)\n",
        "* transfer the training data to the GPU\n",
        "* use the GPU to learn the model\n",
        "* transfer the results to the CPU\n",
        "\n",
        "You will find more explanations here: https://towardsdatascience.com/pytorch-switching-to-the-gpu-a7c0b21e8a99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCEMXZ0YrgLI"
      },
      "outputs": [],
      "source": [
        "# Declare the GPU (its name is \"device\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtYwt3UdgAKR"
      },
      "source": [
        "**Generate data**\n",
        "\n",
        "Below, we define the 3-dimensional Lorenz-63 model given by:\n",
        "\\begin{aligned}{\\frac {\\mathrm {d} x_1}{\\mathrm {d} t}}&=\\sigma (x_2-x_1),\\\\[6pt]{\\frac {\\mathrm {d} x_2}{\\mathrm {d} t}}&=x_1(\\rho -x_3)-x_2,\\\\[6pt]{\\frac {\\mathrm {d} x_3}{\\mathrm {d} t}}&=x_1 x_2-\\beta x_3.\\end{aligned}\n",
        "\n",
        "This ordinary differential equation is using 3 physical parameters and we fix them to $\\sigma=10$, $\\rho=28$ and $\\beta=8/3$. Then, we use Runge-Kutta 4-5 to integrate the model, using the *odeint()* Python function. The integration time is $0.01$ and we generate a sequence of $100$ Lorenz times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE9NWYxUgAKU"
      },
      "outputs": [],
      "source": [
        "# Lorenz-63 dynamical model\n",
        "def Lorenz_63(x, dx, sigma, rho, beta):\n",
        "    dx = zeros((3))\n",
        "    dx[0] = sigma*(x[1]-x[0])\n",
        "    dx[1] = x[0]*(rho-x[2])-x[1]\n",
        "    dx[2] = x[0]*x[1] - beta*x[2]\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMRsuUH9gAKd"
      },
      "outputs": [],
      "source": [
        "from scipy.integrate import odeint\n",
        "\n",
        "# Define the parameters\n",
        "x0 = array([8,0,30]) # initial condition\n",
        "dt = 0.01 # integration time step\n",
        "T = 100 # number of Lorenz-63 times\n",
        "sigma = 10\n",
        "rho = 28\n",
        "beta = 8/3\n",
        "\n",
        "# Generate the Lorenz-63 system\n",
        "x = odeint(Lorenz_63, x0, arange(0.01,T,dt), args=(sigma,rho,beta))\n",
        "time = arange(0.01,T,dt)\n",
        "\n",
        "# Transform array to tensor\n",
        "x = torch.from_numpy(x).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGXvqPR4gAKl"
      },
      "source": [
        "**Visualize data**\n",
        "\n",
        "They are 2 ways of visualizing the Lorenz-63 system. The first is to consider the system as a 3-dimensional time series: we plot each variable as a function of time. The second is the phase-space representation where we plot the relationships between variables in 3D and we track the trajectory along time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdgv6eyFgAKm"
      },
      "outputs": [],
      "source": [
        "# Time series representation\n",
        "plot(time, x)\n",
        "xlabel('Lorenz-63 time', size=20)\n",
        "legend(['x_1','x_2','x_3'], fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9aj957xgAKv"
      },
      "outputs": [],
      "source": [
        "# Phase-space representation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(x[:,0], x[:,1], x[:,2], 'k')\n",
        "ax.set_xlabel('$x_1$', size=20)\n",
        "ax.set_ylabel('$x_2$', size=20)\n",
        "ax.set_zlabel('$x_3$', size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h-wdw3UgAK5"
      },
      "source": [
        "**Create the training and test datasets**\n",
        "\n",
        "Here, we want to predict the Lorenz-63 system at time t using the previous information at time t-0.01. It is thus a regression between the process and himself with a time delay. Here, we create a training (the first 2/3 of time series) and a test dataset (the last part)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_ArvYmZgAK6"
      },
      "outputs": [],
      "source": [
        "# Size of the training set\n",
        "T_train = int(T/dt*2/3)\n",
        "\n",
        "# Training set\n",
        "x_train = x[0:T_train,:]\n",
        "y_train = x[1:T_train+1,:]\n",
        "\n",
        "# Validation set\n",
        "x_test = x[T_train+1:-1,]\n",
        "y_test = x[T_train+2:,]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dNM5eFAgALA"
      },
      "source": [
        "**Question 1:**\n",
        "\n",
        "For the 3 components of the Lorenz-63 system, plot *y_train* as a function of *x_train* and comment the relationship. What is happening if we increase the time delay (currently set to $0.01$) between *x_train* and *y_train*?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrS-PvqzgALC"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "### TO DO ###\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5HgSm2ngALI"
      },
      "source": [
        "**Response:**\n",
        "\n",
        "xxx TO DO xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kofOkmBAgALK"
      },
      "source": [
        "**Question 2:**\n",
        "\n",
        "According to the previous question, propose a linear regression between *y_train* (at time t) and *x_train* (at time t-0.01). It corresponds to a multivariate order 1 autoregressive process noted AR(1), see more details here (https://en.wikipedia.org/wiki/Autoregressive_model).\n",
        "\n",
        "Implement the AR(1) regression using a neural network architecture (see lab session 1 & 2). Call your model *model_AR1*. Can you comment the estimated weights of the resulting neural network?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5QWLcrBgALL"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "### TO DO ###\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1InsCA9bgALn"
      },
      "source": [
        "**Response:**\n",
        "\n",
        "xxx TO DO xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "CE5DQtkygALp"
      },
      "source": [
        "**Question 3:**\n",
        "\n",
        "Now, use a RNN architecture to learn the variations of the time series. To do so, we use the *LSTM()* function from PyTorch. Below, we provide you the dataset and a LSTM class. Here, we recommend that you use GPU acceleration.\n",
        "\n",
        "After implementing and fitting your RNN, have a look at the estimated parameters: what do they represent? See https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn7WGA1JKB3r"
      },
      "source": [
        "First, we have to rearange the dataset used to train the RNN. We also store the dataset on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFszXIWhgALq"
      },
      "outputs": [],
      "source": [
        "# Reshape train and test data (with batch_size = 1)\n",
        "x_train_LSTM = reshape(x_train, (shape(x_train)[0], 1, shape(x_train)[1]))\n",
        "x_test_LSTM = reshape(x_test, (shape(x_test)[0], 1, shape(x_test)[1]))\n",
        "y_train_LSTM = reshape(y_train, (shape(y_train)[0], 1, shape(y_train)[1]))\n",
        "y_test_LSTM = reshape(y_test, (shape(y_test)[0], 1, shape(y_test)[1]))\n",
        "\n",
        "# Store the data on the GPU\n",
        "x_train_LSTM, x_test_LSTM = x_train_LSTM.to(device), x_test_LSTM.to(device)\n",
        "y_train_LSTM, y_test_LSTM = y_train_LSTM.to(device), y_test_LSTM.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubpXnwN0KG2n"
      },
      "source": [
        "Then, we suggest to use the following RNN architecture with one LSTM layer and a fully connected layer with linear activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUTmeXukgALv"
      },
      "outputs": [],
      "source": [
        "# Declare a class for LSTM\n",
        "class LSTM_nn(nn.Module):\n",
        "\n",
        "    # class initialization\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM_nn, self).__init__()\n",
        "        # LSTM layer with 1 hidden layer (hidden_size neurons inside)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1)\n",
        "        # fully connected layer with linear activation\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # function to apply the neural network\n",
        "    def forward(self, x):\n",
        "        out, (h_out, c_out) = self.lstm(x)\n",
        "        y_pred = self.fc(out)\n",
        "        return y_pred, h_out, c_out\n",
        "\n",
        "    # function to apply the neural network (when knowing the hidden state)\n",
        "    def forward_using_last_hidden_layer(self, x, h, c):\n",
        "        out, (h_out, c_out) = self.lstm(x, (h, c))\n",
        "        y_pred = self.fc(out)\n",
        "        return y_pred, h_out, c_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYVNQRKqKe-D"
      },
      "source": [
        "In the next cell, we initialize a LSTM with 10 neurons in the hidden layer. Here, we transfer the LSTM model to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB-z4Tl8gAL2"
      },
      "outputs": [],
      "source": [
        "# Create the LSTM (3 input size for x and 3 output size for y, 10 neurons in the hidden layer)\n",
        "model_RNN = LSTM_nn(3, 10, 3)\n",
        "\n",
        "# Use the GPU to train the model\n",
        "model_RNN.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7HVpOAYLae4"
      },
      "source": [
        "Then, you will have to optimize the LSTM. We suggest to carrefully check the loss function along the epochs. The number of epoch needed to converge can be large, depending on the optimizer and the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAVyLbUML2w-"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "### TO DO ###\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXjoNQ4sgAMH"
      },
      "source": [
        "**Response:**\n",
        "\n",
        "xxx TO DO xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRKQLwsPgAMI"
      },
      "source": [
        "**Question 4:**\n",
        "\n",
        "Compare the predictions of the linear regression AR(1) and the LSTM architecture on the test dataset. Compute the root mean squared errors (for each component) between the predicted values and the truth. Plot also the histograms of the residuals (difference between prediction and truth) for the 2 models. What are the main differences between the ouputs of the AR(1) and the RNN models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPc4WtM4Sjf_"
      },
      "source": [
        "First, you have to transfer the prediction of your LSTM from the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LW068o4Stu4"
      },
      "outputs": [],
      "source": [
        "# Make predictions from the AR1 and LSTM models\n",
        "y_AR1_hat = model_AR1.forward(x_test)\n",
        "y_RNN_hat, h_hat, c_hat = model_RNN.forward(x_test_LSTM)\n",
        "\n",
        "# Transfer data from the GPU\n",
        "y_RNN_hat = y_RNN_hat.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQMuRAyRgAMJ"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "### TO DO ###\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIu6uXWRgAMf"
      },
      "source": [
        "**Response:**\n",
        "\n",
        "xxx TO DO xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nnjsC5r0gAMg"
      },
      "source": [
        "**Question 5:**\n",
        "\n",
        "Now, starting from the last value of *y_test*, use the AR(1) model and the RNN to simulate new data (e.g., a sequence of 1500 time steps). You will need to apply the models recursively. Comment the resulting trajectories: do they look like the original ones?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgn1rBKIgAMi"
      },
      "outputs": [],
      "source": [
        "len_simu = 1500 # length of the simulation\n",
        "x0_AR1 = y_test[-1,:] # initial condition for AR(1)\n",
        "x0_LSTM = y_test_LSTM[-100:,:,:] # initial condition for LSTM (needed to learn the neurons in the hidden layer)\n",
        "\n",
        "# Initialize resulting arrays\n",
        "x_AR1 = torch.zeros((len_simu,3))\n",
        "x_RNN = torch.zeros((len_simu,3)).to(device)\n",
        "x_AR1[0,:] = x0_AR1\n",
        "x_RNN[0,:] = x0_LSTM[-1,:]\n",
        "\n",
        "# Estimate h and c of the LSTM using several Lorenz times\n",
        "x_RNN_tmp, h_hat, c_hat = model_RNN.forward(x0_LSTM)\n",
        "\n",
        "# Loop on the previous prediction\n",
        "for i in range(len_simu-1):\n",
        "  x_AR1[i+1,:] =  ### TO DO ###\n",
        "  x_RNN[i+1,:], h_hat, c_hat = ### TO DO ###\n",
        "\n",
        "# Generate the true run\n",
        "x_truth = odeint(Lorenz_63, y_test[-1,:], arange(0.01,len_simu*dt,dt), args=(sigma,rho,beta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcENboe1gAMp"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "### TO DO ###\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP7lX6DNgAM0"
      },
      "source": [
        "**Response:**\n",
        "\n",
        "xxx TO DO xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaLDSRsfgAM1"
      },
      "source": [
        "**Question 6 (bonus):**\n",
        "\n",
        "Propose and implement a new model to increase the quality of the prediction. We suggest to transform the input and take into account more time lags (i.e., $t-1$, $t-2$, $t-3$) to explain the Lorenz-63 system at time $t$. This can be done using a linear regression and in that case, we are talking about a AR(3) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRKLlJI7A-_1"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "### TO DO ###\n",
        "#############"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "description": "Use Caffe as a generic SGD optimizer to train logistic regression on non-image HDF5 data.",
    "example_name": "Off-the-shelf SGD for classification",
    "include_in_docs": true,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "priority": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}